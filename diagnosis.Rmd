---
title: "CSML1000 Winter 2020, Group 8, Assignment1: Breast Cancer Diagnosis Prediction Model"
author: "Steven Wang, Tarun Bagga, Paul Doucet, Jerry Khidaroo, "
date: "2/15/2020"
output: 
  html_document:
  toc: TRUE
  toc_depth: 2
runtime: shiny
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Load Libraries

```{r, message = FALSE}
# Load packages
library('ggplot2') # visualization
library('ggthemes') # visualization
library('scales') # visualization
library('dplyr') # data manipulation
library('mice') # imputation
library('randomForest') # classification algorithm
library('caret')
library('e1071')
library('shiny')
library('shinythemes')
library('readr')
```

## 1. Business Understanding

- Objective: The Objective of this project is to create a model from the Wisconsin Breast Cancer Data that would allow for a classification prediction of 'B' or 'M' (Benign or Malignant) when given results from tumor measurement studies.

- Project Plan:

- Business Success Criteria: A successful project outcome would be achieved if a model is created that can predict the 'B' or 'M' outcome with a high degree of accuracy.

- Ethical Framework Questions: 
  - How could your system negatively impact individuals? The greatest negative impact would occur for a false positive diagnosis since this could delay treatment and in a life threatening scenario. A false negative would also be negatively impactful but to a lessor degree.
  - Who is most vulnerable and why? The most vulnerable would be patients with a 'M' dianosis not detectable by the model.
  - How much error in predictions can your business accept for this use case? False positives need to be minimised as much as posible. Minimsing False negatives are second in priority.
  - Will you need to explain which input factors had the greatest influence on outputs? Yes. Being able to explain which features have the most influence on outcome is very desirable.
  - Do you need PII or can you provide group-level data? The analysis requires patient level data however any PII can be anonymised

## 2. Data Understanding

- Ethical Framework Questions: 
  - Have you de-identified your data and taken measures to reduce the probability of reidentification? The data is de-identified.
  - Will socially sensitive features like gender or ethnic background influence outputs? No demographic data is present.
  - Are seemingly harmless features like location hiding proxies for socially sensitive features? No demographic data is present. 

#### Get Data File

- The Dataset used is obtained from: https://www.kaggle.com/uciml/breast-cancer-wisconsin-data

```{r, message = FALSE}
# Get dir of opened R file. (Note: Only works for RStudio)
# wdir <- dirname(rstudioapi::getActiveDocumentContext()$path)
# Use relative path instead
wdir <- "."
# show(wdir)

# Concatenate data file subdir and name.
full_file_name <- paste(wdir,"/input/diagnosis_data.csv",sep="")
show(full_file_name)
```

#### Load and check data


```{r, message = FALSE}
# Load full data
diagnosis_data_full = read.csv(full_file_name)

# check data
str(diagnosis_data_full)
summary(diagnosis_data_full)
```

#### Plot the Target against a few of the features to get a feel for the data

```{r, echo=FALSE}
plot(y = diagnosis_data_full$diagnosis, x = diagnosis_data_full$radius_mean)
plot(y = diagnosis_data_full$diagnosis, x = diagnosis_data_full$texture_mean)
plot(y = diagnosis_data_full$diagnosis, x = diagnosis_data_full$concave.points_mean)
```

- Looks like in general 'M' gives higher features values than 'B'

## 3. Data Preparation

#### Split Data into Train and Test Sets

```{r, message = FALSE}
# Split the data into a train set and a test set
diagnosis_data_train <- diagnosis_data_full[1:426,]
diagnosis_data_test <- diagnosis_data_full[427:569,]
```

#### Check diagnosis target variable

```{r, message = FALSE}
prop.table(table(diagnosis_data_train$diagnosis))*100
prop.table(table(diagnosis_data_test$diagnosis))*100
```

- This split is fine as it gives greater importance to 'M' outcomes for training

- Start with a simple Naive Bayes model

## 4. A) Data Modeling - Naive Bayes

```{r, message = FALSE}
diag_nb_model <- naiveBayes(diagnosis_data_train[,-c(1,2)], diagnosis_data_train$diagnosis)
```

##5. A) Data Evaluation - Naive Bayes

#### Test the Naive Bayes model.

```{r, message = FALSE}
diag_nb_pre <- predict(diag_nb_model, diagnosis_data_test[,-c(1,2)])
diag_cm_nb <- confusionMatrix(diag_nb_pre, diagnosis_data_test$diagnosis)
diag_cm_nb
```

- Now Try a lets try some Random Forests

## 4. B) Data Modeling - Random Forest

- Ethical Framework Questions: 
  - Does your use case require a more interpretable algorithm? Accuracy is more impotant than interpretability for this study.
  - Should you be optimizing for a different outcome than accuracy to make your outcomes fairer? Since there is no demographic data, fairness would be hard to determine.
  - Is it possible that a malicious actor has compromised training data and created misleading results? No. The data is from a reputable source.

#### Build a Model based on Mean values

```{r, message = FALSE}
diag_mean_model <- randomForest(factor(diagnosis) ~ radius_mean + texture_mean + perimeter_mean + 
                          + smoothness_mean + compactness_mean + concavity_mean + concave.points_mean +
                            symmetry_mean + fractal_dimension_mean,
                         data = diagnosis_data_train)
diag_mean_model
```

#### Build a Model based on SE values (Standard Error of Mean)

```{r, message = FALSE}
diag_se_model <- randomForest(factor(diagnosis) ~ radius_se + texture_se + perimeter_se + 
                          + smoothness_se + compactness_se + concavity_se + concave.points_se +
                            symmetry_se + fractal_dimension_se,
                         data = diagnosis_data_train)
diag_se_model
```

#### Build a Model based on Worst values (Largest Mean Value)

```{r, message = FALSE}
diag_worst_model <- randomForest(factor(diagnosis) ~ radius_worst + texture_worst + perimeter_worst + 
                          + smoothness_worst + compactness_worst + concavity_worst + concave.points_worst +
                            symmetry_worst + fractal_dimension_worst,
                         data = diagnosis_data_train)
diag_worst_model
```

## 5. B) Data Evaluation - Random Forest

#### Test the Mean Model

```{r, message = FALSE}
pre_mean_rf <- predict(diag_mean_model, diagnosis_data_test[,-c(1,2)])
cm_mean <- confusionMatrix(pre_mean_rf, diagnosis_data_test$diagnosis)
cm_mean
```

#### Test the SE Model

```{r, message = FALSE}
pre_se_rf <- predict(diag_se_model, diagnosis_data_test[,-c(1,2)])
cm_se <- confusionMatrix(pre_se_rf, diagnosis_data_test$diagnosis)
cm_se
```

#### Test the Worst Model

```{r, message = FALSE}
pre_worst_rf <- predict(diag_worst_model, diagnosis_data_test[,-c(1,2)])
cm_worst <- confusionMatrix(pre_worst_rf, diagnosis_data_test$diagnosis)
cm_worst
```

- From these 3 outcomes, the SE model is clearly inferior, so lets look for more details of the other 2 models

#### Examine the Mean Model for feature importance

```{r, message = FALSE}
# Get importance
importance    <- importance(diag_mean_model)
varImportance <- data.frame(Variables = row.names(importance), 
                            Importance = round(importance[ ,'MeanDecreaseGini'],2))

# Create a rank variable based on importance
rankImportance <- varImportance %>%
  mutate(Rank = paste0('#',dense_rank(desc(Importance))))

# Use ggplot2 to visualize the relative importance of variables
ggplot(rankImportance, aes(x = reorder(Variables, Importance),  y = Importance, fill = Importance)) +
    geom_bar(stat='identity') + 
    geom_text(aes(x = Variables, y = 0.5, label = Rank), hjust=0, vjust=0.55, size = 4, colour = 'red') +
    labs(x = 'Variables') + coord_flip() +  theme_few()
```

- Shows the following as most important: concave.points_mean, perimeter_mean, concavity_mean, radius_mean

#### Examine the Worst Model for feature importance

```{r, message = FALSE}
# Get importance
importance    <- importance(diag_worst_model)
varImportance <- data.frame(Variables = row.names(importance), 
                            Importance = round(importance[ ,'MeanDecreaseGini'],2))

# Create a rank variable based on importance
rankImportance <- varImportance %>%
  mutate(Rank = paste0('#',dense_rank(desc(Importance))))

# Use ggplot2 to visualize the relative importance of variables
ggplot(rankImportance, aes(x = reorder(Variables, Importance),  y = Importance, fill = Importance)) +
    geom_bar(stat='identity') + 
    geom_text(aes(x = Variables, y = 0.5, label = Rank), hjust=0, vjust=0.55, size = 4, colour = 'red') +
    labs(x = 'Variables') + coord_flip() +  theme_few()
```

- Shows the following as most important: radius_worst, perimeter_worst, concave.points_worst, concavity_worst

#### Re-Build Model based on only top 4 important Mean values

```{r, message = FALSE}
diag_mean_model <- randomForest(factor(diagnosis) ~ concave.points_mean + perimeter_mean + concavity_mean 
                                + radius_mean,
                         data = diagnosis_data_train)
diag_mean_model
```

#### Re-Build Model based on only top 4 important Worst values (Largest Mean Value)

```{r, message = FALSE}
diag_worst_model <- randomForest(factor(diagnosis) ~ radius_worst + perimeter_worst + concave.points_worst
                             + concavity_worst,
                         data = diagnosis_data_train)
diag_worst_model
```

#### Test the New Mean Model

```{r, message = FALSE}
pre_mean_rf <- predict(diag_mean_model, diagnosis_data_test[,-c(1,2)])
cm_mean <- confusionMatrix(pre_mean_rf, diagnosis_data_test$diagnosis)
cm_mean
```

#### Test the New Worst Model

```{r, message = FALSE}
pre_worst_rf <- predict(diag_worst_model, diagnosis_data_test[,-c(1,2)])
cm_worst <- confusionMatrix(pre_worst_rf, diagnosis_data_test$diagnosis)
cm_worst
```

- So eliminating lessor features slightly reduced the accuracy for both models
- Lets try a model that includes both the mean and worst features


#### Build a Model based Mean + Worst values

```{r, message = FALSE}
diag_mean_worst_model <- randomForest(factor(diagnosis) ~ radius_mean + texture_mean + perimeter_mean + 
                          + smoothness_mean + compactness_mean + concavity_mean + concave.points_mean +
                            symmetry_mean + fractal_dimension_mean + radius_worst + texture_worst + perimeter_worst + 
                          + smoothness_worst + compactness_worst + concavity_worst + concave.points_worst +
                            symmetry_worst + fractal_dimension_worst,
                         data = diagnosis_data_train)
diag_mean_worst_model
```

#### Test the Mean + Worst Model

```{r, message = FALSE}
pre_mean_worst_rf <- predict(diag_mean_worst_model, diagnosis_data_test[,-c(1,2)])
cm_mean_worst <- confusionMatrix(pre_mean_worst_rf, diagnosis_data_test$diagnosis)
cm_mean_worst
```

- This appears to be a slight improvement

- Now lets try a Logistic Regression Model

## 4. C) Data Modeling - Logistic Regression

```{r, message = FALSE}
fit_lg <- trainControl(## 10-fold CV
  method = "cv",
  number = 10,
  savePredictions = TRUE)

diag_lg<-train(diagnosis ~ radius_mean + texture_mean + perimeter_mean + 
                          + smoothness_mean + compactness_mean + concavity_mean + concave.points_mean +
                            symmetry_mean + fractal_dimension_mean + radius_worst + texture_worst + perimeter_worst + 
                          + smoothness_worst + compactness_worst + concavity_worst + concave.points_worst +
                            symmetry_worst + fractal_dimension_worst,
               data=diagnosis_data_train,method="glm",family=binomial(), trControl=fit_lg)
```

## 5. C) Data Evaluation - Logistic Regression

#### Test Logistic Regression Model

```{r, message = FALSE}
diag_lg_pre<-predict(diag_lg,diagnosis_data_test[,-c(1,2)])
cm_lg<-confusionMatrix(diag_lg_pre,diagnosis_data_test$diagnosis)
cm_lg
```

## 6. Deployment

- Ethical Framework Questions: 
  - Can a malicious actor infer information about individuals from your system? No. There is no PII present.
  - Are you able to identify anomalous activity on your system that might indicate a security breach?
  - Do you have a plan to monitor for poor performance on individuals or subgroups?
  - Do you have a plan to log and store historical predictions if a consumer requests access in the future?
  - Have you documented model retraining cycles and can you confirm that a subject’s data has been removed from models?

#### Define UI

```{r, message = FALSE}
ui <- fluidPage(theme = shinytheme("lumen"),
#ui <- fillPage(theme = shinytheme("lumen"),
  titlePanel("Diagnosis Predictor"),
  sidebarLayout(
    sidebarPanel(
      
      # Button to click for generating prediction
      actionButton("predict", "Predict"),
      
      # Inputs
      sliderInput("radius_mean", label = "radius_mean:", min = 1, max = 30, value = 1, step = 0.001),
      sliderInput("texture_mean", label = "texture_mean:", min = 5, max = 50, value = 5, step = 0.01),
      sliderInput("perimeter_mean", label = "perimeter_mean:", min = 25, max = 225, value = 25, step = 0.01),
      sliderInput("area_mean", label = "area_mean:", min = 100, max = 2600, value = 100, step = 0.1),
      sliderInput("smoothness_mean", label = "smoothness_mean:", min = 0.01, max = 0.25, value = 0.01, step = 0.00001),
      sliderInput("compactness_mean", label = "compactness_mean:", min = 0, max = 1, value = 0, step = 0.00001),
      sliderInput("concavity_mean", label = "concavity_mean:", min = 0, max = 1, value = 0, step = 0.00001),
      sliderInput("concave.points_mean", label = "concave.points_mean:", min = 0, max = 25, value = 0, step = 0.00001),
      sliderInput("symmetry_mean", label = "symmetry_mean:", min = 0, max = 5, value = 0, step = 0.00001),
      sliderInput("fractal_dimension_mean", label = "fractal_dimension_mean:", min = 0.01, max = 0.1, value = 0.01, step = 0.00001),
      width = 6
    ),

    # Output: Text Giving a Prediction
    mainPanel(
      h3(textOutput(outputId = "Pred")),
      textOutput(outputId = "ShowStr"),
      tableOutput("Table"), width = 6
    ),
  )
)
```

#### Define server function

```{r, message = FALSE}
server <- function(input, output, session) {

  diagnosis <- "NA"
  
  observeEvent(input$predict, {
    #output$desc <- renderText({
      # desc_text <- input$radius_mean
      # paste("radius_mean: ", desc_text)
    
      data <- reactive({
      req(diagnosis)
      data.frame(diagnosis=NA,
               radius_mean=input$radius_mean,
               texture_mean=input$texture_mean,
               perimeter_mean=input$perimeter_mean,
               area_mean=input$area_mean,
               smoothness_mean=input$smoothness_mean,
               compactness_mean=input$compactness_mean,
               concavity_mean=input$concavity_mean,
               concavity_mean=input$concavity_mean,
               concave.points_mean=input$concave.points_mean,
               symmetry_mean=input$symmetry_mean,
               fractal_dimension_mean=input$fractal_dimension_mean)
      })
      
      str_text <- reactive({
        str(data())
      })
      output$ShowStr <- renderPrint(str_text())

      pred <- reactive({
        predict(diag_mean_model,data())
      })

      output$Pred <- renderPrint(pred())
  })
}
```

#### Run Shiny App

```{r, message = FALSE}
shinyApp(ui = ui, server = server)
```

## References

Yihui Xie, J. J. Allaire, Garrett Grolemund, 2019, R Markdown: The Definitive Guide
https://bookdown.org/yihui/rmarkdown/markdown-syntax.html

Jonathan McPherson, 2016, R Notebooks
https://blog.rstudio.com/2016/10/05/r-notebooks

Adam Kardash, Patricia Kosseim, 2018, Responsible AI in Consumer Enterprise, integrate.ai

Mercedes Ovejero Bruna, 2019, https://www.kaggle.com/mercheovejero/breast-cancer-analysis-real-machine-learning
